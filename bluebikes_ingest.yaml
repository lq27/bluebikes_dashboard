id: bluebikes_ingest
namespace: bluebikes

variables:
  base_url: "https://s3.amazonaws.com/hubway-data"
  url_suffix: "-{% if inputs.year|number < 2018 or (inputs.year|number == 2018 and inputs.month|number <= 4) %}hubway{% else %}bluebikes{% endif %}-tripdata"
  zip_suffix: ".zip"
  csv_suffix: ".csv"
  filename: '{{ inputs.year }}{{ inputs.month }}{{ vars.url_suffix }}'
  expected_headers:
    - "ride_id"
    - "rideable_type"
    - "started_at"
    - "ended_at"
    - "start_station_name"
    - "start_station_id"
    - "end_station_name"
    - "end_station_id"
    - "start_lat"
    - "start_lng"
    - "end_lat"
    - "end_lng"
    - "member_casual"
    - 'tripduration'
    - 'birth year'
    - 'gender'
    - 'bikeid'

inputs:
  - id: year
    type: STRING
    defaults: '2015'
  - id: month
    type: SELECT
    values: ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']
    defaults: '01'


pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"

tasks:
  - id: set_label
    type: io.kestra.plugin.core.execution.Labels
    labels:
      month: render(inputs.month)
      year: render(inputs.year)

  - id: get_zipfile
    type: io.kestra.plugin.core.http.Download
    uri: "{{ vars.base_url }}/{{ render(vars.filename) }}{{vars.zip_suffix}}"

  - id: extract_zip
    type: io.kestra.plugin.compress.ArchiveDecompress
    algorithm: ZIP
    from: "{{ outputs.get_zipfile.uri }}"

  - id: validate_and_transform_schema
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: ghcr.io/kestra-io/pydata:latest
    script: |
      import pandas as pd
      import re
      import os
      import sys
      
      # Get the expected headers from the input
      expected_headers = {{ vars.expected_headers }}
      input_file = "{{ outputs.extract_zip.files[render(vars.filename) ~ vars.csv_suffix ] }}"
      transformed_output_file = 'transformed_output_file.csv'
      
      try:
          # Read the CSV file
          df = pd.read_csv(input_file, low_memory=False)
          
          # Store original columns for reporting
          original_columns = set(df.columns)
          
          # Create mapping dictionary for column renaming
          column_mapping = {}
          
          # Possible variations of column names
          variations = {
              'ride_id': ['ride_id', 'rideid', 'ride id', 'trip_id', 'tripid', 'trip id'],
              'rideable_type': ['rideable_type', 'rideable type', 'bike_type', 'bike type', 'biketype'],
              'started_at': ['started_at', 'started at', 'start_time', 'start time', 'starttime', 'starttime'],
              'ended_at': ['ended_at', 'ended at', 'end_time', 'end time', 'stoptime', 'endtime'],
              'start_station_name': ['start_station_name', 'start station name', 'from_station_name', 'from station name'],
              'start_station_id': ['start_station_id', 'start station id', 'from_station_id', 'from station id'],
              'end_station_name': ['end_station_name', 'end station name', 'to_station_name', 'to station name'],
              'end_station_id': ['end_station_id', 'end station id', 'to_station_id', 'to station id'],
              'start_lat': ['start_lat', 'start lat', 'start_latitude', 'start latitude', 'start station latitude'],
              'start_lng': ['start_lng', 'start lng', 'start_longitude', 'start longitude', 'start station longitude'],
              'end_lat': ['end_lat', 'end lat', 'end_latitude', 'end latitude', 'end station latitude'],
              'end_lng': ['end_lng', 'end lng', 'end_longitude', 'end longitude', 'end station longitude'],
              'member_casual': ['member_casual', 'member casual', 'usertype', 'user_type', 'user type', 'customer_type']
          }
          
          # Find the best match for each column
          matched_columns = set()
          for expected_col in expected_headers:
              # Check if the column exists as is
              if expected_col in df.columns:
                  matched_columns.add(expected_col)
                  continue
                  
              # Try to find variations
              found_match = False
              if expected_col in variations:
                  for variation in variations[expected_col]:
                      if variation in df.columns:
                          column_mapping[variation] = expected_col
                          matched_columns.add(variation)
                          found_match = True
                          break
          
          # Log the mapping for debugging
          print(f"Column mapping: {column_mapping}")
          
          # Apply the column mapping
          df = df.rename(columns=column_mapping)

          default_values = {
            'rideable_type': 'classic_bike',
             # Add more custom defaults here if needed
          }
          
          # Check for missing columns after renaming
          current_columns = set(df.columns)
          missing_columns = set(expected_headers) - current_columns
          
          # Fill missing columns with defaults or None if not specified
          for col in missing_columns:
            default_value = default_values.get(col, None)
            df[col] = default_value
            print(f"Added missing column: {col} with default value: {default_value}")
          
          # Check for any unexpected columns that remain
          final_columns = set(df.columns)
          unexpected_columns = final_columns - set(expected_headers)
          
          if unexpected_columns:
            print("Schema validation failed!")
            print(f"Unexpected columns: {unexpected_columns}")
            sys.exit(1)
          
          # Reorder columns to match expected_headers
          df_reordered = df[expected_headers]
          
          # Save the transformed file
          df_reordered.to_csv(transformed_output_file, index=False)
          print(f"Transformed CSV saved to: {transformed_output_file}")
          
          # Output for Kestra
          sys.exit(0)
              
      except Exception as e:
          print(f"Error transforming schema: {e}")
          sys.exit(1)
    outputFiles:
      - "transformed_output_file.csv"

  - id: upload_to_gcs
    type: io.kestra.plugin.gcp.gcs.Upload
    from: "{{ outputs.validate_and_transform_schema.outputFiles['transformed_output_file.csv'] }}"
    to: "gs://{{kv('GCP_BUCKET_NAME')}}/{{ inputs.year }}/{{ inputs.month }}/{{render(vars.filename)}}{{vars.csv_suffix}}"

  - id: load_staging_to_bigquery
    type: io.kestra.plugin.gcp.bigquery.Load
    destinationTable: "{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.staging_table"
    from: "{{ outputs.validate_and_transform_schema.outputFiles['transformed_output_file.csv'] }}"
    format: CSV
    writeDisposition: WRITE_TRUNCATE  # overwrites existing table, if any
    autodetect: true
    csvOptions:
      skipLeadingRows: 1

  - id: unique_id_bq_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE OR REPLACE TABLE `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.{{ render(vars.filename) }}`
      AS
      SELECT
        MD5(CONCAT(
          COALESCE(CAST(ride_id AS STRING), ""),
          COALESCE(CAST(started_at AS STRING), ""),
          COALESCE(CAST(start_station_id AS STRING), ""),
          COALESCE(CAST(end_station_id AS STRING), ""),
          COALESCE(CAST(member_casual AS STRING), ""), 	
          COALESCE(CAST(rideable_type AS STRING), "")
        )) AS unique_row_id,
        "{{ render(vars.filename) }}" AS filename,
        *
      FROM `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.staging_table`;

  - id: all_trip_data
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      CREATE TABLE IF NOT EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.all_trip_data`
      (
        unique_row_id BYTES,
        fileid STRING, 
        ride_id	STRING,
        rideable_type	STRING,	
        started_at	TIMESTAMP,	
        ended_at	TIMESTAMP,	
        start_station_name	STRING,	
        start_station_id	STRING,	
        end_station_name	STRING,	
        end_station_id	STRING,	
        start_lat	FLOAT64,	
        start_lng	FLOAT64,	
        end_lat	FLOAT64,	
        end_lng	FLOAT64,	
        member_casual	STRING,	
        tripduration	STRING,	
        `birth year`	STRING,	
        gender	STRING,	
        bikeid	STRING
      )

  - id: merge_to_all_data
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      MERGE `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.all_trip_data` T
      USING `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.{{ render(vars.filename) }}` S
      ON T.unique_row_id = S.unique_row_id
      WHEN NOT MATCHED THEN INSERT ROW

  - id: success_delete_gcs_file
    type: io.kestra.plugin.gcp.gcs.Delete
    uri: "gs://{{kv('GCP_BUCKET_NAME')}}/{{ inputs.year }}/{{ inputs.month }}/{{render(vars.filename)}}{{vars.csv_suffix}}"

  - id: success_delete_staging_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: "DROP TABLE IF EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.staging_table`"

  - id: success_delete_unique_id_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: "DROP TABLE IF EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.{{ render(vars.filename) }}`"
    
  - id: success_purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: To avoid cluttering your storage, we will remove the downloaded files

errors:

  - id: failed_purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: "Cleanup files after failure"

  - id: failed_delete_staging_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: "DROP TABLE IF EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.staging_table`"

  - id: failed_delete_unique_id_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: "DROP TABLE IF EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_BQ_DATASET')}}.{{ render(vars.filename) }}`"

  - id: failed_delete_gcs_file
    type: io.kestra.plugin.gcp.gcs.Delete
    uri: "gs://{{kv('GCP_BUCKET_NAME')}}/{{ inputs.year }}/{{ inputs.month }}/{{render(vars.filename)}}{{vars.csv_suffix}}"
    errorOnMissing: false


